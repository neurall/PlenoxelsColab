{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plenoxels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neurall/PlenoxelsColab/blob/main/Plenoxels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first version seems to work at least on Colab Pro providing 16g gpu and with high mem option enabled(32g ram) to fit and train M60 dataset (Colab Pro is 10$ a month which is price of 1 hamburger)  \n",
        "<br>\n",
        "\n",
        "You can fork this colab at https://github.com/neurall/PlenoxelsColab/blob/main/Plenoxels.ipynb\n",
        "\n",
        "It is an attempt to make running in Colab cloud GPU possible for the wonderfull breakthrough that is Plenoxels made by fantastic team of incredibly talented  \n",
        "\n",
        "# Plenoxels Authors: (Alex Yu * Sara Fridovich-Keil * Matthew Tancik Qinhong Chen Benjamin Recht Angjoo Kanazawa UC Berkeley * Equal contribution)  \n",
        "\n",
        "Github:            https://github.com/sxyu/svox2  \n",
        "Website and video: https://alexyu.net/plenoxels  \n",
        "arXiv:             https://arxiv.org/abs/2112.05131  \n",
        "\n",
        "Their achievement of shortening training from days to minutes is nothing short of extreaordinary. Incredible feat and wonderfull insight.\n",
        "\n",
        "But due to recent shortage of gpus on market simple instant and easily accesible GPU cloud based workflow seems to be more important to fast research iteration than ever.\n",
        "<br>\n",
        "\n",
        "This colab will currently sadly not yet work on free colab with large datasets as Tank M60 one (but in theory can soon, see idea proposed below?) instance.\n",
        "  \n",
        "And niether will it probably reasonably well work on any gpu commonly at your pc like 10g 3080 or 11g 1080 (Yes. you can remove upsampling to higher res then 256 in configs and it will fit in gpu mem but the output was then unusable and noisy, at least for me) \n",
        "\n",
        "# Free colab has just 12g ram 11g (P8) gpu or 15g (t4?) gpu and this svox2 variant needs at least 27g ram and at least 16g gpu (p100?) for training datasets of similar size as cool M60 Tank dataset from my observations so far  \n",
        "\n",
        "Perhaps These Larger datasets could be in theory splitable to smaller ones that will fit Free colab or your local 1080/3080.  \n",
        "We could use estimated camera locations per image from bundle adjustment colmap files that datasets already have.  \n",
        "<br>\n",
        "Say you wanna train on 10gb 3080. And you can't fit and train at once.\n",
        "<br>\n",
        "What if split the photos from big dataset to three smaller groups of near photos/datasets.ABC  \n",
        "<br>\n",
        "pass 1 Train on images from both A and B camera pos ranges  to voxel mesh1  \n",
        "pass 2 Train on images from both B and C camera pos ranges  to voxel mesh2  \n",
        "pass 3 Train on images from both C and A camera pos ranges  to voxel mesh3  \n",
        "<br> \n",
        "cut duplicite coordinate pass1 pass2 pass3 voxels or use weighted interpolation ? \n",
        "<br>\n",
        "Or.  \n",
        "sIf you have more gpus. Split dataset depending on how many gpus you have with what gpu mem sizes  how deep in upsampling in config res you wanna go 512? 640 or more?) each fitting 4-24g gpus again by estimated camera locations from bundle adjustment colmap files that datasets already have?  \n",
        "\n",
        "Say you have 2 x 11g gpus. Split dataset photos to 2 x 2 chunks ABCD by estimated cam positions that datasets already have  \n",
        "<br>\n",
        "Then train on as many gpus in paralell as you want  \n",
        "<br>\n",
        "pass1 train individual scene fragments\n",
        "<br>\n",
        "gpu1:AB gpu2:CD  \n",
        "<br>\n",
        "pass2 to solve underrepresented voxels from border regions of chunks to allow good merge of seams \n",
        "<br>\n",
        "gpu1:BC gpu2:DA    \n",
        "<br>\n",
        "merge pass1 and pass2  \n",
        "<br>\n",
        "Datasets:\n",
        "NeRF-synthetic and front facing llff: \n",
        "https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1  \n",
        "Processed Tanks and temples dataset (with background): \n",
        "https://drive.google.com/file/d/1PD4oTP4F8jTtpjd_AQjCsL4h8iYFCyvO  \n",
        "Real Lego capture: \n",
        "https://drive.google.com/file/d/1PG-KllCv4vSRPO7n5lpBjyTjlUyT8Nag  \n",
        "Pretrained checkpoints (good if you cant train?): \n",
        "https://drive.google.com/drive/folders/1SOEJDw8mot7kf5viUK9XryOAmZGe_vvE  \n",
        "\n"
      ],
      "metadata": {
        "id": "Zbg9iHgVQjfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sadly. my 3080 while it more than twice as fast as Colab Pro is sitting iddle due to just 10g gpu mem\n",
        "# So let's see what gpu colab given us (16g gpu mem is minumum for M60 tank sample dataset training and is given on Colab Pro instances).\n",
        "from psutil import virtual_memory\n",
        "\n",
        "gpuname = !nvidia-smi -L\n",
        "gpuname = str(gpuname).replace('PCIE-','').split('Tesla ')[1].split(' ')[0]\n",
        "\n",
        "print(gpuname)\n",
        "\n",
        "gpu_info = !nvidia-smi \n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('BEWARE!!! Not connected to a GPU. Nothing will work')\n",
        "else:\n",
        "  gpu_mem = int(str(gpu_info).split('MiB / ')[1].split('MiB')[0])\n",
        "  if gpu_mem < 16000:\n",
        "    print(gpu_info,'\\n\\nBEWARE!!! GPUs with less than 16g will fail with out of memory on M60 training near the end of training.')\n",
        "  else:\n",
        "    print('Great. You got '+str(gpu_mem)+' gpu mem')\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('BEWARE!!! Not using a high-RAM runtime of Colab Pro. M60 training will fail on out of system memory which is just 12 g on Colab Free')\n",
        "else:\n",
        "  print('Great. You are using a high-RAM runtime '+str(int(ram_gb))+'Gb ! 27g is needed to train example M60 Tank dataset\\n')\n",
        "\n",
        "# one day colab will hopefully move to python 3.8 and higher\n",
        "!python --version\n",
        "\n",
        "# for faster compiles later\n",
        "!apt install -y -qq  ninja-build  &> /dev/null\n",
        "\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "%env MAX_JOBS={cores}\n",
        "\n",
        "# make sure that colmap is ready if needed to work on our own images later\n",
        "!apt install colmap &> /dev/null\n",
        "colmap_version = !colmap -h\n",
        "print (colmap_version[0])\n",
        "\n",
        "# There is no point in slow conda and especially his multi env dance in this throwavay one use colab env anyway and most packages from environment.yml are installed already.\n",
        "# Thing is. If we stick to colabs py 3.7 then biggest and most complex ones like pytorch cudatoolkit and most of environment.yml requirements is already installed \n",
        "# so lets install just 4 missing packages. This way env can be restarted very fast without endless and useless huge long reinstalls\n",
        "\n",
        "!pip install imageio-ffmpeg &> /dev/null\n",
        "!pip install ipdb           &> /dev/null\n",
        "!pip install lpips          &> /dev/null\n",
        "!pip install pymcubes       &> /dev/null\n",
        "\n",
        "# this is key to fit in gpu mem. helps to lower gpu fragmentation alloc waste and in turn allows fitting 16g gpu mem typically present in cloud gpus as max\n",
        "# ie I was last time 20mb short instead of 2g short on gpu mem\n",
        "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:21"
      ],
      "metadata": {
        "id": "4Eho4tgYSAv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxGfk51EjgiC"
      },
      "outputs": [],
      "source": [
        "#comment capture out to see cell outputs first time. I personally consider it unecessary super long distracting spam afterwards and cell execution done checkmark is all I need.\n",
        "%%capture \n",
        "import os\n",
        "%cd '/content'\n",
        "if not os.path.exists('svox2'):\n",
        "  !git clone https://github.com/sxyu/svox2.git  &> /dev/null\n",
        "\n",
        "  # patch and make py sources again compatible with CoLab (python 3.7) by removing 3.8 specific syntax from two lines\n",
        "  !sed -E -i \"s/\\{minv=:/minv=\\{minv:/g\"    /content/svox2/opt/opt.py \n",
        "  !sed -E -i \"s/\\{meanv=:/meanv=\\{meanv:/g\" /content/svox2/opt/opt.py \n",
        "  !sed -E -i \"s/\\{maxv=:/maxv=\\{maxv:/g\"    /content/svox2/opt/opt.py \n",
        "  !sed -E -i \"s/\\{minv=:/minv=\\{minv:/g\"    /content/svox2/opt/render_imgs.py \n",
        "  !sed -E -i \"s/\\{meanv=:/meanv=\\{meanv:/g\" /content/svox2/opt/render_imgs.py \n",
        "  !sed -E -i \"s/\\{maxv=:/maxv=\\{maxv:/g\"    /content/svox2/opt/render_imgs.py \n",
        "\n",
        "%cd svox2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make env restarts faster. It is perhaps good idea to use gdrive to cache once compiled pytorch wheel and checkpoint after training \n",
        "# so next time colab runtime resets and forgets all files next bootstrap is fast\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kaYiQzjzApuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Because compile and install pytorch wheel is soooo sloooow. We compile it just once,\n",
        "# cache on gdrive and then just reinstall on each runtime restart\n",
        "# we cache multiple whl files for each  gpu we encounter and save whls to gdrive \n",
        "# in dirs with gpu names since they can and will vary\n",
        "import os\n",
        "from google.colab import files\n",
        "%cd /content/svox2\n",
        "\n",
        "\n",
        "# grab currenly assigned gpu model name (P100-16gb etc.)\n",
        "gpuname = !nvidia-smi -L\n",
        "gpuname = str(gpuname).replace('PCIE-','').split('Tesla ')[1].split(' ')[0]\n",
        "whlname = 'svox2-0.0.1.dev0+sphtexcub.lincolor.fast-cp37-cp37m-linux_x86_64.whl'\n",
        "whlpath = '/content/drive/MyDrive/'+gpuname+'/'+whlname\n",
        "print(gpuname)\n",
        "\n",
        "# compile this whl just once first time to obtain and cache it on gdrive\n",
        "if not os.path.exists(whlpath):\n",
        "  !apt install ninja-build\n",
        "  %env MAX_JOBS=4\n",
        "  !python setup.py bdist_wheel &> /dev/null\n",
        "  !mkdir /content/drive/MyDrive/{gpuname}\n",
        "  !cp ./dist/{whlname} {whlpath}\n",
        "\n",
        "# install cached whl next time env is reset to skip costly recompilation\n",
        "if os.path.exists(whlpath):\n",
        "  !pip install {whlpath} --force-reinstall"
      ],
      "metadata": {
        "id": "gUqq8knldYD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'TanksAndTempleBG'\n",
        "experiment   = 'M60'"
      ],
      "metadata": {
        "id": "y7_nqfcVSzzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download super cool tank dataset if not data dir already present\n",
        "import os\n",
        "%cd /content/svox2\n",
        "# Datasets: from gdrive folder where id is last folder name in url: https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1\n",
        "gdrive_ids={'TanksAndTempleBG':'1PD4oTP4F8jTtpjd_AQjCsL4h8iYFCyvO', #\n",
        "            'nerf_llff_data'  :'16VnMcF1KJYxN9QId6TClMsZRahHNMW5g',\n",
        "            'nerf_real_360'   :'1jzggQ7IPaJJTKx9yLASWHrX8dXHnG5eB',\n",
        "            'nerf_synthetic'  :'18JxhpWD-4ZmuFKLzKlAw-w5PpzZxXOcG'}\n",
        "\n",
        "if not os.path.exists('/content/svox2/data/'+dataset_name):\n",
        "  console_output = !gdown --id {gdrive_ids[dataset_name]} \n",
        "  downloaded_filename = str(console_output).split('To: /content/svox2/')[1].split('\\'')[0]\n",
        "\n",
        "  !mkdir data  &> /dev/null\n",
        "  if '.zip' in downloaded_filename:\n",
        "    !unzip  -q {downloaded_filename} -d data\n",
        "  \n",
        "  if '.tar.gz' in downloaded_filename:\n",
        "    !tar   -xf {downloaded_filename} -C data\n",
        "\n",
        "  # if needed, unify root data dir name and subdir structure since some tar based datasets \n",
        "  # are in aditional subdirs with unique names. let's have just data root dir\n",
        "  if os.path.exists('/content/svox2/data/'+dataset_name):\n",
        "    !mv data/{dataset_name}/* data/\n",
        "\n",
        "  # remove huge downloaded no longer needed file. \n",
        "  !rm  -f  {downloaded_filename}"
      ],
      "metadata": {
        "id": "qxqjozeh2oy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: \n",
        "# enable this optional step to upload and colab calibrate your own images \n",
        "# and prepare dataset for svox2 to extract voxels via svox2\n",
        "# \n",
        "# %cd /content/svox2/opt/\n",
        "\n",
        "# !bash scripts/proc_colmap.sh video_to_obj\n",
        "\n",
        "# !python scripts/run_colmap.py video_to_obj\n",
        "# !python colmap2nsvf.py video_to_obj/sparse/0\n",
        "# !python create_split.py -y video_to_obj\n",
        "# !pip install nerfvis\n",
        "# !python scripts/view_data.py video_to_obj"
      ],
      "metadata": {
        "id": "nqzaX9yKtRY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launch actual training (remember. 27g ram and 16g gpu for TanksAndTempleBG M60 dataset is needed if selected above)\n",
        "%cd /content/svox2/opt\n",
        "custom_configs={'TanksAndTempleBG':'tnt',\n",
        "                'nerf_llff_data'  :'llff',\n",
        "                'nerf_real_360'   :'llff',\n",
        "                'nerf_synthetic'  :'syn'}\n",
        "!./launch.sh {experiment} 0 ../data/{experiment} -c configs/{custom_configs[dataset_name]}.json"
      ],
      "metadata": {
        "id": "eG6WJ_aiV7KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since training runs detached we can peek at output on demand here when needed\n",
        "#\n",
        "# STOP THIS MANUALLY when training is done!!! when you see \"* Final eval and save\" line in output)\n",
        "# tail -f on itself will never end!!! \n",
        "%cd /content/svox2/opt\n",
        "!tail -f ckpt/{experiment}/log"
      ],
      "metadata": {
        "id": "RzGaIuArWuBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see our final resulting trained checkpoint file size when done . official one is 4G\n",
        "!ls -la /content/svox2/opt/ckpt/{experiment}/ckpt.npz"
      ],
      "metadata": {
        "id": "DvAZ6a-xsaEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backup our expensively trained experiment checkpoint to gdrive for future rendering tests even outside of colab if needed \n",
        "# or restore from gdrive if training pass above failed or was skipped\n",
        "import os\n",
        "%cd /content/svox2/opt\n",
        "!mkdir /content/drive/MyDrive/ckpt &> /dev/null\n",
        "\n",
        "# if training was succesfull and produced npz\n",
        "if os.path.exists('/content/svox2/opt/ckpt/'+experiment+'/ckpt.npz'):\n",
        "  print('backing ckpt '+experiment+' to gdrive')\n",
        "  !cp -rf /content/svox2/opt/ckpt/{experiment} /content/drive/MyDrive/ckpt\n",
        "\n",
        "# if not, or was skipped, let's restore checkpoint of this experiment from our gdrive if any is found\n",
        "else:\n",
        "  if os.path.exists('/content/drive/MyDrive/ckpt/'+experiment+'/ckpt.npz'):\n",
        "    print('loading ckpt '+experiment+' from gdrive')\n",
        "    !cp -rf /content/drive/MyDrive/ckpt/{experiment} /content/svox2/opt/ckpt"
      ],
      "metadata": {
        "id": "oGJD2CrXbHOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Sadly even thou colab pro has 16gb, it is still just 16g gpu)\n",
        "# SO. If the damn training for too large datasets keeps failing with: out of memory \n",
        "#\n",
        "# Then. Luckily. We can still at least try download already trained checkpoints (thx. to paper authors ;D yay)\n",
        "# And at least start playing with rendering images from various angles\n",
        "#\n",
        "# BUT BEWARE this is 11g gz download if enabled\n",
        "# AND ALSO: if 3-4 downloads happen per 24h of this 11g large file from gdrive, which is highly plausible given usage in this colab. \n",
        "# Then google colab will block further downloads via this gdown api at this particular day for 24h\n",
        "# If that happens. You can still supposedly download it via browser and curl/wget or upload it here by other means I guess?\n",
        "\n",
        "%cd /content/svox2/opt\n",
        "import os\n",
        "\n",
        "# change this to True to enable pretrained checkpoints download. beware again 11g\n",
        "if False:\n",
        "  !mv ckpt ckpt_our\n",
        "  if not os.path.exists('ckpt_tnt.tar.gz'): \n",
        "    !gdown --id 1v9xb5Sd3ulofwNUynC71I_fdwnSLnFhS \n",
        "  !tar -xvf ckpt_tnt.tar.gz  &> /dev/null\n",
        "  !rm  -rf  ckpt_tnt.tar.gz # delete this huge 11g file as fast as possible\n",
        "  !ls -la tnt_equirectlin_fasttv_autoscale/M60/ckpt.npz # check ckpt size for m60. should be around 4g\n",
        "  !mv tnt_equirectlin_fasttv_autoscale ckpt"
      ],
      "metadata": {
        "id": "eIUHJyMrQNxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally lets synthetize some sample images from various angles (typically 360 orbit around object) from trained checkpoint / model\n",
        "%cd /content/svox2/opt\n",
        "!python -u render_imgs.py ckpt/{experiment} ../data/{experiment} "
      ],
      "metadata": {
        "id": "53KZA1Miqzp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer 3mb of jpgs in zip is more practical than 40mb pngs or hard to inspect mp4 with too aggresive compression\n",
        "%%capture \n",
        "%cd /content/svox2/opt/ckpt/{experiment}/test_renders\n",
        "!for i in *.png; do ffmpeg -i \"$i\" \"${i%.*}.jpg\" &> /dev/null ; done \n",
        "!find . -type f -iname \\*.png -delete\n",
        "%cd /content/svox2/opt\n",
        "!zip -rq images.zip /content/svox2/opt/ckpt/{experiment}/test_renders &> /dev/null "
      ],
      "metadata": {
        "id": "T_DzNH_vj5Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy resulting images to special in html cell visible dir. since html gives bigger output preview flexibility\n",
        "%%capture \n",
        "!cp /content/svox2/opt/ckpt/{experiment}/test_renders/*.* /usr/local/share/jupyter/nbextensions/ &> /dev/null\n",
        "!cp images.zip /usr/local/share/jupyter/nbextensions/ &> /dev/null"
      ],
      "metadata": {
        "id": "y9F8BfCNZUxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show some sample synthetized images\n",
        "%%html\n",
        "<a href src='/nbextensions/images.zip' download>Download Images</a><br>\n",
        "<img width=\"100%\" src='/nbextensions/0000.jpg' />\n",
        "<img width=\"100%\" src='/nbextensions/0020.jpg' />\n",
        "<img width=\"100%\" src='/nbextensions/0031.jpg' />"
      ],
      "metadata": {
        "id": "ZzAMVhVQYZX2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}